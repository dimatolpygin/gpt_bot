import OpenAI from 'openai';
import { config } from '../config/index.js';
import { File } from 'node:buffer';

const openai = new OpenAI({ apiKey: config.OPENAI_API_KEY });

const SYSTEM = {
  role: 'system',
  content: 'Ð¢Ñ‹ ÑƒÐ¼Ð½Ñ‹Ð¹ Ð¸ Ð¿Ð¾Ð»ÐµÐ·Ð½Ñ‹Ð¹ AI-Ð°ÑÑÐ¸ÑÑ‚ÐµÐ½Ñ‚. ÐžÑ‚Ð²ÐµÑ‡Ð°Ð¹ Ð½Ð° ÑÐ·Ñ‹ÐºÐµ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ. Ð‘ÑƒÐ´ÑŒ Ñ‡Ñ‘Ñ‚ÐºÐ¸Ð¼, ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¼ Ð¸ ÑÐ¾Ð´ÐµÑ€Ð¶Ð°Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ð¼.',
};

const REASONING_MODELS = new Set([
  'gpt-5', 'gpt-5-mini', 'gpt-5-nano',
  'gpt-5.2', 'gpt-5.2-pro',
]);

export const THINKING_EMOJI = {
  none:  'ðŸ’­',
  low:   'ðŸ§ ',
  medium:'ðŸ§ ðŸ§ ',
  high:  'ðŸ§ ðŸ§ ðŸ§ ',
  xhigh: 'ðŸ§ âš¡',
};

const wrapError = (err) => {
  if (err.status === 429) throw new Error('ÐŸÑ€ÐµÐ²Ñ‹ÑˆÐµÐ½ Ð»Ð¸Ð¼Ð¸Ñ‚ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð² OpenAI. ÐŸÐ¾Ð´Ð¾Ð¶Ð´Ð¸Ñ‚Ðµ.');
  if (err.status === 401) throw new Error('ÐÐµÐ²ÐµÑ€Ð½Ñ‹Ð¹ OpenAI API ÐºÐ»ÑŽÑ‡.');
  if (err.status === 404) throw new Error(`ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð° Ð² Ð²Ð°ÑˆÐµÐ¼ Ð°ÐºÐºÐ°ÑƒÐ½Ñ‚Ðµ OpenAI.`);
  if (err.status === 400) throw new Error(`ÐžÑˆÐ¸Ð±ÐºÐ° Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°: ${err.message}`);
  throw err;
};

// â”€â”€ Streaming Ñ‡ÐµÑ€ÐµÐ· Chat Completions API â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
export const streamChat = async (messages, modelId, onChunk, options = {}) => {
  try {
    const { thinkingLevel = 'none' } = options;
    const model = modelId || config.OPENAI_MODEL;
    const payload = [
      { role: 'system', content: SYSTEM.content },
      ...messages,
    ];

    const params = {
      model,
      messages: payload,
      stream: true,
    };

    if (REASONING_MODELS.has(model) && thinkingLevel !== 'none') {
      params.reasoning_effort = thinkingLevel;
    }

    const stream = await openai.chat.completions.create(params);
    let fullText = '';
    for await (const chunk of stream) {
      const delta = chunk.choices?.[0]?.delta?.content || '';
      if (delta) {
        fullText += delta;
        if (onChunk) await onChunk(delta, fullText);
      }
    }
    return fullText;
  } catch (err) {
    wrapError(err);
  }
};

// â”€â”€ Streaming Ñ Ð²ÐµÐ±-Ð¿Ð¾Ð¸ÑÐºÐ¾Ð¼ (Responses API) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
export const webSearchChat = async (messages, modelId, onChunk, options = {}) => {
  try {
    const { thinkingLevel = 'none' } = options;
    const model = modelId || config.OPENAI_MODEL;
    const payload = [
      { role: 'system', content: SYSTEM.content },
      ...messages,
    ];

    const params = {
      model,
      input: payload,
      stream: true,
      tools: [{ type: 'web_search_preview' }],
    };

    if (REASONING_MODELS.has(model) && thinkingLevel !== 'none') {
      params.reasoning = { effort: thinkingLevel };
    }

    const stream = await openai.responses.create(params);
    let fullText = '';
    for await (const event of stream) {
      const delta = event?.delta?.text ?? event?.delta ?? '';
      if (typeof delta === 'string' && delta) {
        fullText += delta;
        if (onChunk) await onChunk(delta, fullText);
      }
    }
    return fullText;
  } catch (err) {
    wrapError(err);
  }
};

// â”€â”€ ÐÐ½Ð°Ð»Ð¸Ð· Ñ„Ð¾Ñ‚Ð¾ (vision) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
export const analyzePhoto = async (imageUrl, caption, modelId) => {
  try {
    const model = modelId || 'gpt-4o';
    const response = await openai.chat.completions.create({
      model,
      messages: [{
        role: 'user',
        content: [
          { type: 'image_url', image_url: { url: imageUrl } },
          { type: 'text', text: caption || 'ÐŸÐ¾Ð´Ñ€Ð¾Ð±Ð½Ð¾ Ð¾Ð¿Ð¸ÑˆÐ¸ Ñ‡Ñ‚Ð¾ Ð½Ð° ÑÑ‚Ð¾Ð¼ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¸.' },
        ],
      }],
    });
    return response.choices[0]?.message?.content ?? '';
  } catch (err) {
    wrapError(err);
  }
};

const getMimeType = (fileName) => {
  const ext = fileName?.split('.').pop()?.toLowerCase();
  const types = {
    pdf: 'application/pdf',
    txt: 'text/plain',
    md: 'text/markdown',
    csv: 'text/csv',
    json: 'application/json',
    js: 'text/javascript',
    ts: 'text/typescript',
    py: 'text/x-python',
    docx: 'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
  };
  return types[ext] || 'application/octet-stream';
};

export const analyzeFile = async (fileBuffer, fileName, caption, modelId) => {
  let uploaded;
  try {
    const file = new File([fileBuffer], fileName, { type: getMimeType(fileName) });
    uploaded = await openai.files.create({ file, purpose: 'user_data' });

    const response = await openai.responses.create({
      model: modelId || 'gpt-4o',
      input: [
        {
          role: 'user',
          content: [
            { type: 'input_file', file_id: uploaded.id },
            { type: 'input_text', text: caption || 'ÐŸÑ€Ð¾Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐ¹ ÑÑ‚Ð¾Ñ‚ Ñ„Ð°Ð¹Ð» Ð¸ Ð¾Ð¿Ð¸ÑˆÐ¸ ÐµÐ³Ð¾ ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ð¼Ð¾Ðµ.' },
          ],
        },
      ],
    });

    return response.output_text ?? '';
  } catch (err) {
    wrapError(err);
  } finally {
    if (uploaded?.id) {
      await openai.files.del(uploaded.id).catch(() => {});
    }
  }
};

export const codeInterpreterChat = async (messages, modelId) => {
  try {
    const response = await openai.responses.create({
      model: modelId || 'gpt-4o',
      input: messages,
      tools: [
        { type: 'code_interpreter', container: { type: 'auto' } },
      ],
      tool_choice: 'auto',
    });

    const text = response.output_text || '';
    const files = [];

    for (const item of response.output || []) {
      if (item.type === 'code_interpreter_call') {
        for (const out of item.outputs || []) {
          if (out.type === 'file' && out.file_id) {
            try {
              const fileData = await openai.files.content(out.file_id);
              const buffer = Buffer.from(await fileData.arrayBuffer());
              const filename = out.filename || `output_${out.file_id.slice(-6)}.txt`;
              files.push({ name: filename, buffer });
              await openai.files.del(out.file_id).catch(() => {});
            } catch (e) {
              console.error('[CodeInterp] file download error:', e.message);
            }
          }
        }
      }
    }

    return { text, files };
  } catch (err) {
    wrapError(err);
  }
};
