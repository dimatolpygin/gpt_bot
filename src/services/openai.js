import OpenAI from 'openai';
import { config } from '../config/index.js';
import { File } from 'node:buffer';

const openai = new OpenAI({ apiKey: config.OPENAI_API_KEY });

const SYSTEM = {
  role: 'system',
  content: 'Ð¢Ñ‹ ÑƒÐ¼Ð½Ñ‹Ð¹ Ð¸ Ð¿Ð¾Ð»ÐµÐ·Ð½Ñ‹Ð¹ AI-Ð°ÑÑÐ¸ÑÑ‚ÐµÐ½Ñ‚. ÐžÑ‚Ð²ÐµÑ‡Ð°Ð¹ Ð½Ð° ÑÐ·Ñ‹ÐºÐµ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ. Ð‘ÑƒÐ´ÑŒ Ñ‡Ñ‘Ñ‚ÐºÐ¸Ð¼, ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¼ Ð¸ ÑÐ¾Ð´ÐµÑ€Ð¶Ð°Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ð¼.',
};

const REASONING_MODELS = new Set([
  'gpt-5', 'gpt-5-mini', 'gpt-5-nano',
  'gpt-5.2', 'gpt-5.2-pro',
]);

const FILE_KEYWORDS = [
  'ÑÐ¾Ð·Ð´Ð°Ð¹ Ñ„Ð°Ð¹Ð»','ÑÐ´ÐµÐ»Ð°Ð¹ Ñ„Ð°Ð¹Ð»','ÑÐ³ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐ¹ Ñ„Ð°Ð¹Ð»','ÑÐ¾Ð·Ð´Ð°Ð¹ pdf','ÑÐ´ÐµÐ»Ð°Ð¹ pdf',
  'ÐºÐ¾Ð½Ð²ÐµÑ€Ñ‚Ð¸Ñ€ÑƒÐ¹ Ð² pdf','ÑÐ¾Ð·Ð´Ð°Ð¹ xlsx','ÑÐ¾Ð·Ð´Ð°Ð¹ csv','ÑÐ¾Ð·Ð´Ð°Ð¹ docx','ÑÐ¾Ñ…Ñ€Ð°Ð½Ð¸ Ð² Ñ„Ð°Ð¹Ð»',
  'Ð²Ñ‹Ð³Ñ€ÑƒÐ·Ð¸ Ð² Ñ„Ð°Ð¹Ð»','ÑÐºÐ°Ñ‡Ð°Ñ‚ÑŒ Ñ„Ð°Ð¹Ð»','ÑÐ¾Ð·Ð´Ð°Ð¹ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñƒ','Ð¿Ð¾ÑÑ‚Ñ€Ð¾Ð¹ Ð³Ñ€Ð°Ñ„Ð¸Ðº','Ð½Ð°Ñ€Ð¸ÑÑƒÐ¹ Ð³Ñ€Ð°Ñ„Ð¸Ðº',
  'generate file','create file','make file','export to','save as pdf','create csv',
  'create xlsx','create chart','plot','draw chart'
];

export const needsCodeInterpreter = (text = '') => {
  const lower = text.toLowerCase();

  const hasFormat = /\b(pdf|xlsx|xls|csv|docx|doc|txt|png|svg|zip|pptx)\b/.test(lower);
  const hasAction = /(ÑÐ¾Ð·Ð´Ð°Ð¹|ÑÐ¾Ð·Ð´Ð°Ð¹Ñ‚Ðµ|ÑÐ´ÐµÐ»Ð°Ð¹|ÑÐ´ÐµÐ»Ð°Ð¹Ñ‚Ðµ|ÑÐ³ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐ¹|ÑÐ³ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐ¹Ñ‚Ðµ|Ð½Ð°Ð¿Ð¸ÑˆÐ¸|Ð½Ð°Ð¿Ð¸ÑˆÐ¸Ñ‚Ðµ|Ð²Ñ‹Ð³Ñ€ÑƒÐ·Ð¸|Ð²Ñ‹Ð³Ñ€ÑƒÐ·Ð¸Ñ‚Ðµ|ÐºÐ¾Ð½Ð²ÐµÑ€Ñ‚Ð¸Ñ€ÑƒÐ¹|ÐºÐ¾Ð½Ð²ÐµÑ€Ñ‚Ð¸Ñ€ÑƒÐ¹Ñ‚Ðµ|Ð¿Ñ€ÐµÐ¾Ð±Ñ€Ð°Ð·ÑƒÐ¹|ÑÐºÑÐ¿Ð¾Ñ€Ñ‚Ð¸Ñ€ÑƒÐ¹|Ð¾Ñ‚Ð¿Ñ€Ð°Ð²ÑŒ|ÑÐºÐ°Ñ‡Ð°Ñ‚ÑŒ|Ð¿Ð¾ÑÑ‚Ñ€Ð¾Ð¹|Ð½Ð°Ñ€Ð¸ÑÑƒÐ¹|create|generate|make|export|convert|draw|plot|build)/i.test(lower);
  const hasPhrases = /(ÑÐ¾Ð·Ð´Ð°Ð¹(Ñ‚Ðµ)? Ñ„Ð°Ð¹Ð»|ÑÐ´ÐµÐ»Ð°Ð¹(Ñ‚Ðµ)? Ñ„Ð°Ð¹Ð»|ÑÐ³ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐ¹(Ñ‚Ðµ)? Ñ„Ð°Ð¹Ð»|Ð½Ð°Ñ€Ð¸ÑÑƒÐ¹(Ñ‚Ðµ)? (Ð´Ð¸Ð°Ð³Ñ€Ð°Ð¼Ð¼Ñƒ|Ð³Ñ€Ð°Ñ„Ð¸Ðº|Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñƒ)|Ð¿Ð¾ÑÑ‚Ñ€Ð¾Ð¹(Ñ‚Ðµ)? Ð³Ñ€Ð°Ñ„Ð¸Ðº|create file|generate file|make file|draw chart|plot graph)/i.test(lower);

  return hasPhrases || (hasFormat && hasAction);
};

export const THINKING_EMOJI = {
  none:  'ðŸ’­',
  low:   'ðŸ§ ',
  medium:'ðŸ§ ðŸ§ ',
  high:  'ðŸ§ ðŸ§ ðŸ§ ',
  xhigh: 'ðŸ§ âš¡',
};

const wrapError = (err) => {
  if (err.status === 429) throw new Error('ÐŸÑ€ÐµÐ²Ñ‹ÑˆÐµÐ½ Ð»Ð¸Ð¼Ð¸Ñ‚ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð² OpenAI. ÐŸÐ¾Ð´Ð¾Ð¶Ð´Ð¸Ñ‚Ðµ.');
  if (err.status === 401) throw new Error('ÐÐµÐ²ÐµÑ€Ð½Ñ‹Ð¹ OpenAI API ÐºÐ»ÑŽÑ‡.');
  if (err.status === 404) throw new Error(`ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð° Ð² Ð²Ð°ÑˆÐµÐ¼ Ð°ÐºÐºÐ°ÑƒÐ½Ñ‚Ðµ OpenAI.`);
  if (err.status === 400) throw new Error(`ÐžÑˆÐ¸Ð±ÐºÐ° Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°: ${err.message}`);
  throw err;
};

const getRetryAfter = (err) => {
  const header = err.headers?.['retry-after']
    ?? err.headers?.['Retry-After']
    ?? err?.response?.headers?.get?.('retry-after')
    ?? err?.response?.headers?.['retry-after'];
  const value = parseInt(header, 10);
  return Number.isNaN(value) ? 10 : value;
};

const withRetry = async (fn, maxRetries = 3) => {
  for (let attempt = 0; attempt < maxRetries; attempt++) {
    try {
      return await fn();
    } catch (err) {
      const is429 = err.status === 429 || err.message?.includes('429');
      if (!is429 || attempt >= maxRetries - 1) throw err;
      const retryAfter = Math.min(getRetryAfter(err) * 1000, 60000);
      console.warn(`[API] 429 rate limit, retry ${attempt + 1}/${maxRetries} after ${retryAfter / 1000}s`);
      await new Promise(r => setTimeout(r, retryAfter));
    }
  }
};

// â”€â”€ Streaming Ñ‡ÐµÑ€ÐµÐ· Chat Completions API â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
export const streamChat = async (messages, modelId, onChunk, options = {}) => {
  try {
    const { thinkingLevel = 'none' } = options;
    const model = modelId || config.OPENAI_MODEL;
    const hasSystem = messages.length > 0 && messages[0].role === 'system';
    const systemMessage = hasSystem ? messages[0] : { role: 'system', content: SYSTEM.content };
    const restMessages = hasSystem ? messages.slice(1) : messages;
    const payload = [systemMessage, ...restMessages];

    const params = {
      model,
      messages: payload,
      stream: true,
    };

    if (REASONING_MODELS.has(model) && thinkingLevel !== 'none') {
      params.reasoning_effort = thinkingLevel;
    }

    const stream = await withRetry(() => openai.chat.completions.create(params));
    let fullText = '';
    for await (const chunk of stream) {
      const delta = chunk.choices?.[0]?.delta?.content || '';
      if (delta) {
        fullText += delta;
        if (onChunk) await onChunk(delta, fullText);
      }
    }
    return fullText;
  } catch (err) {
    wrapError(err);
  }
};

// â”€â”€ Streaming Ñ Ð²ÐµÐ±-Ð¿Ð¾Ð¸ÑÐºÐ¾Ð¼ (Responses API) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
export const webSearchChat = async (messages, modelId, onChunk, options = {}) => {
  try {
    const { thinkingLevel = 'none' } = options;
    const model = modelId || config.OPENAI_MODEL;
    const hasSystem = messages.length > 0 && messages[0].role === 'system';
    const systemMessage = hasSystem ? messages[0] : { role: 'system', content: SYSTEM.content };
    const restMessages = hasSystem ? messages.slice(1) : messages;
    const payload = [systemMessage, ...restMessages];

    const params = {
      model,
      input: payload,
      stream: true,
      tools: [{ type: 'web_search_preview' }],
    };

    if (REASONING_MODELS.has(model) && thinkingLevel !== 'none') {
      params.reasoning = { effort: thinkingLevel };
    }

    const stream = await withRetry(() => openai.responses.create(params));
    let fullText = '';
    for await (const event of stream) {
      const delta = event?.delta?.text ?? event?.delta ?? '';
      if (typeof delta === 'string' && delta) {
        fullText += delta;
        if (onChunk) await onChunk(delta, fullText);
      }
    }
    return fullText;
  } catch (err) {
    wrapError(err);
  }
};

// â”€â”€ ÐÐ½Ð°Ð»Ð¸Ð· Ñ„Ð¾Ñ‚Ð¾ (vision) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
export const analyzePhoto = async (imageUrl, caption, modelId) => {
  try {
    const model = modelId || 'gpt-4o';
    const response = await openai.chat.completions.create({
      model,
      messages: [{
        role: 'user',
        content: [
          { type: 'image_url', image_url: { url: imageUrl } },
          { type: 'text', text: caption || 'ÐŸÐ¾Ð´Ñ€Ð¾Ð±Ð½Ð¾ Ð¾Ð¿Ð¸ÑˆÐ¸ Ñ‡Ñ‚Ð¾ Ð½Ð° ÑÑ‚Ð¾Ð¼ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¸.' },
        ],
      }],
    });
    return response.choices[0]?.message?.content ?? '';
  } catch (err) {
    wrapError(err);
  }
};

const getMimeType = (fileName) => {
  const ext = fileName?.split('.').pop()?.toLowerCase();
  const types = {
    pdf: 'application/pdf',
    txt: 'text/plain',
    md: 'text/markdown',
    csv: 'text/csv',
    json: 'application/json',
    js: 'text/javascript',
    ts: 'text/typescript',
    py: 'text/x-python',
    docx: 'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
  };
  return types[ext] || 'application/octet-stream';
};

export const analyzeFile = async (fileBuffer, fileName, caption, modelId) => {
  let uploaded;
  try {
    const file = new File([fileBuffer], fileName, { type: getMimeType(fileName) });
    uploaded = await openai.files.create({ file, purpose: 'user_data' });

    const response = await openai.responses.create({
      model: modelId || 'gpt-4o',
      input: [
        {
          role: 'user',
          content: [
            { type: 'input_file', file_id: uploaded.id },
            { type: 'input_text', text: caption || 'ÐŸÑ€Ð¾Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐ¹ ÑÑ‚Ð¾Ñ‚ Ñ„Ð°Ð¹Ð» Ð¸ Ð¾Ð¿Ð¸ÑˆÐ¸ ÐµÐ³Ð¾ ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ð¼Ð¾Ðµ.' },
          ],
        },
      ],
    });

    return response.output_text ?? '';
  } catch (err) {
    wrapError(err);
  } finally {
    if (uploaded?.id) {
      await openai.files.del(uploaded.id).catch(() => {});
    }
  }
};

export const codeInterpreterChat = async (messages, modelId) => {
  console.log('[CodeInterp] starting, model:', modelId);
  try {
    const response = await openai.responses.create({
      model: modelId || 'gpt-4o',
      input: messages,
      tools: [
        { type: 'code_interpreter', container: { type: 'auto' } },
      ],
      tool_choice: 'auto',
    });

    console.log('[CodeInterp] output items:', response.output?.map(i => i.type));

    const text = response.output_text || '';
    const files = [];

    const containerIds = new Set();
    for (const item of response.output || []) {
      if (item.type === 'code_interpreter_call' && item.container_id) {
        containerIds.add(item.container_id);
      }
    }

    console.log('[CodeInterp] unique container_ids:', Array.from(containerIds));

    for (const containerId of containerIds) {
      try {
        const filesList = await openai.containers.files.list(containerId);
        console.log('[CodeInterp] files in container:', filesList.data?.length || 0);

        for (const fileInfo of filesList.data || []) {
          if (fileInfo.source !== 'assistant') {
            console.log('[CodeInterp] skip user file:', fileInfo.path);
            continue;
          }

          if (!fileInfo.bytes) {
            console.warn('[CodeInterp] bytes: null for file:', fileInfo.path, '(likely non-ASCII filename)');
          }

          try {
            const fileContent = await openai.containers.files.content.retrieve(
              containerId,
              fileInfo.id
            );

            const buffer = Buffer.from(await fileContent.arrayBuffer());
            if (buffer.length === 0) {
              console.warn('[CodeInterp] empty buffer for:', fileInfo.path);
              continue;
            }
            const filename = fileInfo.path.replace(/^\/mnt\/data\//, '') || `file_${Date.now()}.txt`;
            console.log('[CodeInterp] file ready:', filename, buffer.length, 'bytes');
            files.push({ name: filename, buffer });
          } catch (downloadErr) {
            console.error('[CodeInterp] file download error:', downloadErr.message);
          }
        }
      } catch (listErr) {
        console.error('[CodeInterp] container files.list error:', listErr.message);
      }
    }

    console.log('[CodeInterp] done, files:', files.length);
    return { text, files };
  } catch (err) {
    wrapError(err);
  }
};

export const transcribeVoice = async (audioBuffer, options = {}) => {
  const {
    fileName = 'voice_message.ogg',
    contentType = 'audio/ogg',
    language = 'auto',
  } = options;

  const transcription = await openai.audio.transcriptions.create({
    file: new File([audioBuffer], fileName, { type: contentType }),
    model: 'whisper-1',
    language,
  });

  return transcription?.text ?? '';
};

export { openai };

export async function generateImage(prompt, size = '1024x1024') {
  console.log('[Image] generating:', prompt.slice(0, 80));

  const response = await openai.images.generate({
    model: 'gpt-image-1.5',
    prompt,
    n: 1,
    size,
    quality: 'medium',
    output_format: 'png',
  });

  const item = response.data?.[0];
  if (!item) throw new Error('No image data');
  if (item.b64_json) {
    return Buffer.from(item.b64_json, 'base64');
  }
  if (item.url) {
    const res = await fetch(item.url);
    return Buffer.from(await res.arrayBuffer());
  }
  throw new Error('No image data in response');
}
